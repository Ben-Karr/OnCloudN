{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8325de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from fastai.vision.all import *\n",
    "from assets.chiputility import *\n",
    "from loguru import logger\n",
    "import typer\n",
    "import skimage.morphology # remove small objects from prediction\n",
    "import torch \n",
    "import gc # collect gpu memory\n",
    "import os # to remove tmp folder\n",
    "\n",
    "def get_chips(path):\n",
    "    \"\"\"\n",
    "    Get chip-paths from folder, make sure that only chip-folders are included.\n",
    "    Assumed structure is:\n",
    "    -path\n",
    "        -chip_0\n",
    "            -B2.tif\n",
    "            -B3.tif\n",
    "            -...\n",
    "        -chip_1\n",
    "        ...\n",
    "    \"\"\"\n",
    "    potential_chips = list(path.iterdir())\n",
    "    chips_paths = L(chip for chip in potential_chips if chip.is_dir())\n",
    "    return chips_paths.attrgot('name')\n",
    "\n",
    "def save_probs(probs, i_batch, bs, chips, path, k):\n",
    "    \"\"\"\n",
    "    Saves the predicted probability for every pixel to be a cloud pixel to a .npy file.\n",
    "    Gets the chip and filename by looking it up in the chip-path source, assumes the chip-path source to be sorted.\n",
    "    This is used to be able to ensemble different models predictions, while limiting memory usage.\n",
    "    \"\"\"\n",
    "    i = i_batch * bs\n",
    "    for j in range(probs.shape[0]):\n",
    "        chip_id = chips[i+j].stem\n",
    "        #fn = (path / (chip_id + f'_{k}')).with_suffix('.npy')\n",
    "        fn = (path / chip_id).with_suffix('.npy')\n",
    "        \n",
    "        if fn.exists():\n",
    "            old_probs = np.load(fn)\n",
    "            probs[j] += old_probs\n",
    "        np.save(fn, probs[j])\n",
    "\n",
    "def save_imgs(tmp_path, preds_path, n_models):\n",
    "    for fn in tmp_path.iterdir():\n",
    "        preds = np.load(fn)\n",
    "        mask = (preds / n_models) > 0.5\n",
    "        mask = Image.fromarray(mask.astype(np.uint8))\n",
    "        \n",
    "        mask.save((preds_path / fn.name).with_suffix('.tif'))\n",
    "\n",
    "        \n",
    "## Augmentations that are necessary to load the pretrained models.\n",
    "class OnlyVizAlbumentationsTransform(Transform):\n",
    "    split_idx = 0\n",
    "    def __init__(self, aug):\n",
    "        self.aug = aug\n",
    "    def encodes(self, x):\n",
    "        if len(x.shape) > 2:\n",
    "            non_viz_channels = x[...,3:]\n",
    "            viz_aug = self.aug(image=x[...,:3].astype(np.float32))['image']\n",
    "            return np.concatenate([viz_aug,non_viz_channels], axis = -1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class SegmentationAlbumentationsTransform(ItemTransform):\n",
    "    split_idx = 0\n",
    "    def __init__(self, aug): \n",
    "        self.aug = aug\n",
    "    def encodes(self, x):\n",
    "        augs = []\n",
    "        for img,mask in x:\n",
    "            augs.append(tuple(self.aug(image=img, mask=mask).values()))\n",
    "        return augs\n",
    "\n",
    "class TransposeTransform(ItemTransform):\n",
    "    def encodes(self, x):\n",
    "        transposed = []\n",
    "        for img in x:\n",
    "            transposed.append(TensorImage(img[0].transpose(2,0,1)).float(),)\n",
    "        return transposed\n",
    "\n",
    "class FormatTransform(ItemTransform):\n",
    "    def __init__(self, return_type):\n",
    "        self.return_type = return_type\n",
    "    def encodes(self, x):\n",
    "        return self.return_type([TensorImage(x[0].permute(0,3,1,2)).float(), TensorMask(x[1]).long()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bb1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(mask, cutoff = 0.5, min_object_size = 100.):\n",
    "    \"\"\"\n",
    "    Crerates instance segmentation from semantic segmentation. Removes objects that are smaller than \n",
    "    `min_object_size` and returns semantic seg. mask.\n",
    "    \"\"\"\n",
    "    label_mask = skimage.morphology.label(mask > cutoff)\n",
    "\n",
    "    labels = set(label_mask.flatten())\n",
    "    if len(labels) == 1: ## len == 1 means all mask or no mask, nothing to remove\n",
    "        return mask\n",
    "    labels.remove(0) ## remove background label, else all background becomes object\n",
    "    prediction = np.zeros(shape = (512, 512), dtype = np.uint8)\n",
    "    for l in labels:\n",
    "        p = (label_mask == l).astype(np.uint8)\n",
    "        if p.sum() >= min_object_size:\n",
    "            prediction += p\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ccb72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(tmp_path, preds_path, n_models):\n",
    "    \"\"\"\n",
    "    Loads the (summed up) probabilities from .npy file. Takes the mean (by deviding by the number of used models) and\n",
    "    applies the post processing and smoothing. Saves the binary mask as .tif.\n",
    "    \"\"\"\n",
    "    for fn in tmp_path.iterdir():\n",
    "        preds = np.load(fn)\n",
    "        mask_arr = (preds / n_models) > 0.5 # loads sum (ensemble) of predicted probs for cloud\n",
    "        \n",
    "        mask_arr = post_process(mask_arr, min_object_size = 50) # remove small objects\n",
    "        mask = Image.fromarray(mask_arr.astype(np.uint8))\n",
    "                \n",
    "        smth_mask = mask.filter(ImageFilter.ModeFilter(size = 10)) # smooth mask\n",
    "        smth_mask.save((preds_path / fn.name).with_suffix('.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9df729ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-06 14:03:01.069 | INFO     | __main__:<module>:2 - Setting up stuff\n",
      "2022-02-06 14:03:01.071 | INFO     | __main__:<module>:13 - Setting up tfms to load learner\n",
      "2022-02-06 14:03:01.072 | INFO     | __main__:<module>:39 - Create folder for predictions\n",
      "2022-02-06 14:03:01.073 | INFO     | __main__:<module>:44 - Create Data source\n"
     ]
    }
   ],
   "source": [
    "## Setup paths\n",
    "logger.info('Setting up stuff')\n",
    "ROOT_DIRECTORY = Path('.')\n",
    "test_path = ROOT_DIRECTORY / 'data' / 'test_features'\n",
    "preds_path = ROOT_DIRECTORY / 'predictions'\n",
    "model_paths = [ROOT_DIRECTORY /'assets/model_old_split', \n",
    "               ROOT_DIRECTORY / 'assets/model_new_split',\n",
    "               ROOT_DIRECTORY / 'assets/model_fp16',\n",
    "              ]\n",
    "tmp_path = ROOT_DIRECTORY / 'tmp'\n",
    "\n",
    "## Setup tfms\n",
    "logger.info('Setting up tfms to load learner')\n",
    "viz_augs_list = A.Compose([\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=0.2,\n",
    "        sat_shift_limit=0.2,\n",
    "        val_shift_limit=0.2,\n",
    "        p = 0.5\n",
    "    ),\n",
    "    #A.Normalize(max_pixel_value = 1),\n",
    "    A.RandomBrightnessContrast(),\n",
    "])\n",
    "\n",
    "geom_augs_list = A.Compose([\n",
    "    A.Flip(),\n",
    "    #A.RandomCrop(440, 440),\n",
    "    A.RandomGridShuffle(grid = (2,2), p = 0.3),\n",
    "    A.CoarseDropout(mask_fill_value = 0),\n",
    "    ])\n",
    "\n",
    "viz_augs_tfms = OnlyVizAlbumentationsTransform(viz_augs_list)\n",
    "geom_augs_tfms = SegmentationAlbumentationsTransform(geom_augs_list)\n",
    "transpose_tfm = TransposeTransform()\n",
    "format_tfm = FormatTransform(tuple)\n",
    "\n",
    "## Make folder for predictions\n",
    "## Should be build already\n",
    "logger.info('Create folder for predictions')\n",
    "preds_path.mkdir(exist_ok = True, parents=True)\n",
    "tmp_path.mkdir(exist_ok = True)\n",
    "\n",
    "## Create DataFrame to fit learners transform pipeline\n",
    "logger.info('Create Data source')\n",
    "test_df = pd.DataFrame({'chip_id': get_chips(test_path)})\n",
    "chips = Chips(test_path, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fca67474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-06 14:03:01.369 | INFO     | __main__:<module>:3 - Load Learner 1 of 3\n",
      "2022-02-06 14:03:01.532 | INFO     | __main__:<module>:10 - Predict on 34 chips with batch size of 8\n",
      "2022-02-06 14:03:01.783 | INFO     | __main__:<module>:15 - Process batch nr: 0\n",
      "2022-02-06 14:03:03.487 | INFO     | __main__:<module>:22 - Finished predicting 34 probabilities with Learner 1.\n",
      "2022-02-06 14:03:03.563 | INFO     | __main__:<module>:3 - Load Learner 2 of 3\n",
      "2022-02-06 14:03:03.714 | INFO     | __main__:<module>:10 - Predict on 34 chips with batch size of 8\n",
      "2022-02-06 14:03:03.954 | INFO     | __main__:<module>:15 - Process batch nr: 0\n",
      "2022-02-06 14:03:05.713 | INFO     | __main__:<module>:22 - Finished predicting 34 probabilities with Learner 2.\n",
      "2022-02-06 14:03:05.787 | INFO     | __main__:<module>:3 - Load Learner 3 of 3\n",
      "2022-02-06 14:03:05.919 | INFO     | __main__:<module>:10 - Predict on 34 chips with batch size of 8\n",
      "2022-02-06 14:03:06.132 | INFO     | __main__:<module>:15 - Process batch nr: 0\n",
      "2022-02-06 14:03:07.819 | INFO     | __main__:<module>:22 - Finished predicting 34 probabilities with Learner 3.\n",
      "2022-02-06 14:03:07.897 | INFO     | __main__:<module>:27 - Ensemble predictions and save as .tif\n",
      "2022-02-06 14:03:11.888 | INFO     | __main__:<module>:29 - Clean up\n",
      "2022-02-06 14:03:11.893 | INFO     | __main__:<module>:31 - Done\n"
     ]
    }
   ],
   "source": [
    "for k, model_path in enumerate(model_paths):\n",
    "    ## Load pretrained learner with GPU\n",
    "    logger.info(f'Load Learner {k+1} of {len(model_paths)}')\n",
    "    learn = load_learner(model_path, cpu = False)\n",
    "    learn.dls.loaders[0].before_batch = Pipeline(transpose_tfm) ## Workaround to get required order of dimensions\n",
    "    test_dl = learn.dls.test_dl(chips.paths, bs = 8)\n",
    "    ## Get predictions and transform to masks, save as binary .tif\n",
    "    bs = test_dl.bs\n",
    "    logger.info(f'Predict on {len(test_df)} chips with batch size of {bs}')\n",
    "    with torch.no_grad():\n",
    "        n_inst = 0\n",
    "        for i,b in enumerate(test_dl):\n",
    "            if (i % 50 == 0):\n",
    "                logger.info(f'Process batch nr: {i}')\n",
    "            preds = learn.model(b).cpu()\n",
    "            soft_preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "            prob_preds = soft_preds[:,1,...].numpy().astype(np.float16)\n",
    "            save_probs(prob_preds, i, bs, chips.paths, tmp_path, k = k)\n",
    "\n",
    "            n_inst += preds.shape[0]\n",
    "    logger.info(f'Finished predicting {n_inst} probabilities with Learner {k+1}.')\n",
    "    del(learn)\n",
    "    del(test_dl)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "logger.info(f'Ensemble predictions and save as .tif')\n",
    "save_imgs(tmp_path, preds_path, len(model_paths))\n",
    "logger.info('Clean up')\n",
    "os.system('rm -rf tmp')\n",
    "logger.info(f'Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc27190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
